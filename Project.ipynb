{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Class Project: Bilingual Speech Recognition for Personal Assistants**\n",
    "\n",
    "**Project Member:**  \n",
    "Tharnarch Thoranisttakul (Omz), Student ID: 63340500025  \n",
    "FIBO, KMUTT\n",
    "\n",
    "As of now (Apr, 8 2023), the current stable release for DeepSpeech is 0.9.3. Therefore, we will use the DeepSpeech version 0.9.3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References:**\n",
    "\n",
    "https://www.section.io/engineering-education/speech-to-text-transcription-model-using-deep-speech/  \n",
    "https://deepspeech.readthedocs.io/en/latest/Python-API.html\n",
    "\n",
    "Audio Sample Durations:  \n",
    "https://stackoverflow.com/questions/42558461/how-long-should-audio-samples-be-for-music-speech-discrimination  \n",
    "https://github.com/NVIDIA/NeMo/issues/1459  \n",
    "https://mozilla.github.io/deepspeech-playbook/DATA_FORMATTING.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 15:48:03.516986: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-10 15:48:04.043303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 15:48:04.552859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-10 15:48:04.573189: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-10 15:48:04.573324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "from deepspeech import Model\n",
    "from datasets import load_dataset, config\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import wave\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the models and creating alphabet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSpeech 0.9.3\n",
    "# Model, Scorer and Alphabet paths\n",
    "model_file_path = 'models/deepspeech-0.9.3-models.pbmm'\n",
    "scorer_file_path = 'models/deepspeech-0.9.3-models.scorer'\n",
    "\n",
    "if not os.path.exists(model_file_path):\n",
    "    # Acoustic Model\n",
    "    !wget -P models https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.pbmm\n",
    "if not os.path.exists(scorer_file_path):\n",
    "    # Language Model\n",
    "    !wget -P models https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/omzlette/miniconda3/envs/bsr/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: deepspeech==0.9.3 in /home/omzlette/miniconda3/envs/bsr/lib/python3.9/site-packages (0.9.3)\n",
      "Requirement already satisfied: numpy>=1.19.4 in /home/omzlette/miniconda3/envs/bsr/lib/python3.9/site-packages (from deepspeech==0.9.3) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "# Install DeepSpeech 0.9.3 using pip\n",
    "!pip install deepspeech==0.9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_path = 'models/alphabet.txt'\n",
    "\n",
    "withTH = False\n",
    "\n",
    "enAlpList = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "             'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "thAlpList = ['ก', 'ข', 'ฃ', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช', 'ซ', 'ฌ', 'ญ', 'ฎ', 'ฏ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท', 'ธ', 'น', 'บ',\n",
    "             'ป', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ', 'ม', 'ย', 'ร', 'ล', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ฮ', 'ฯ', 'ะ', 'ั', 'า', 'ำ', 'ิ', 'ี', 'ึ', 'ื', 'ุ', 'ู', 'ฺ',\n",
    "             'เ', 'แ', 'โ', 'ใ', 'ไ', 'ๅ', 'ๆ', '็', '่', '้', '๊', '๋', '์', 'ํ', '๎']\n",
    "sortedAlpList = sorted(enAlpList + thAlpList + [\"'\", '\"', ',', '.', '?', '!']) if withTH else sorted(enAlpList + [\"'\", '\"', ',', '.', '?', '!'])\n",
    "\n",
    "# Generate alphabet.txt (Every time so it's the correct one)\n",
    "with open(alphabet_path, 'w') as f:\n",
    "    for i in sortedAlpList:\n",
    "        f.write(i + '\\n')\n",
    "    f.write(' ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/omzlette/miniconda3/envs/bsr/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to /home/omzlette/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_12_0 (/home/omzlette/BSR-Project/data/datasets/mozilla-foundation___common_voice_12_0/en/12.0.0/dd534e3c6006ee4b577c176df4a8ef23bced8b3150a3b64d2d0a7a5e3f942efb)\n"
     ]
    }
   ],
   "source": [
    "# Login to HuggingFace\n",
    "!huggingface-cli login --token=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\n",
    "\n",
    "# Set download path and cache path\n",
    "config.DOWNLOADED_DATASETS_PATH = \"/media/omzlette/2ndSSD/CommonVoice_Corpus/data\"\n",
    "config.HF_CACHE_HOME = os.path.expanduser(\"~/BSR-Project/data\")\n",
    "config.HF_DATASETS_CACHE = os.path.join(config.HF_CACHE_HOME, \"datasets\")\n",
    "config.HF_METRICS_CACHE = os.path.join(config.HF_CACHE_HOME, \"metrics\")\n",
    "config.HF_MODULES_CACHE = os.path.join(config.HF_CACHE_HOME, \"modules\")\n",
    "\n",
    "en_cv13 = load_dataset(\"mozilla-foundation/common_voice_12_0\", \"en\", split='train')\n",
    "# th_cv13 = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"th\", split=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to various references above on duration of audio samples and the DeepSpeech Playbook itself, we will make our audio samples have the length between 10 - 20 seconds. However, before we clean up our data, we will check whether the data has duration specified or not. If not, then we will check the sampling rate if they are the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': Value(dtype='string', id=None),\n",
       " 'path': Value(dtype='string', id=None),\n",
       " 'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\n",
       " 'sentence': Value(dtype='string', id=None),\n",
       " 'up_votes': Value(dtype='int64', id=None),\n",
       " 'down_votes': Value(dtype='int64', id=None),\n",
       " 'age': Value(dtype='string', id=None),\n",
       " 'gender': Value(dtype='string', id=None),\n",
       " 'accent': Value(dtype='string', id=None),\n",
       " 'locale': Value(dtype='string', id=None),\n",
       " 'segment': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_cv13.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the sampling rate is at 48000 but, in this case, we will present another way to check the sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05434561d3543308afea9e7a653e968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/986897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "    num_rows: 986897\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique_sampling_rates = set()\n",
    "\n",
    "# def update_unique_sampling_rates(batch):\n",
    "#     for audio in batch['audio']:\n",
    "#         unique_sampling_rates.add(audio['sampling_rate'])\n",
    "\n",
    "# en_cv13.map(update_unique_sampling_rates, batched=True, batch_size=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{48000}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique_sampling_rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, every audio file's sampling rate is 48000.\n",
    "![Unique Sampling Rates EN](https://github.com/omzlette/BSR-EN-TH/blob/main/pic/unique_sampling_rate_en.png?raw=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, the dataset already gave us the audio array and the sampling rate. We can use it to calculate the <u>duration</u> using the **length of the array divided by the sampling rate**.\n",
    "\n",
    "Here's the code for calculating and creating a new duration feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_duration(batch):\n",
    "#     batch['duration'] = np.array([data['array'].shape[0]/data['sampling_rate'] for data in batch['audio']])\n",
    "#     return batch\n",
    "\n",
    "# en_cv13 = en_cv13.map(calculate_duration, batched=True, batch_size=500)\n",
    "\n",
    "# en_cv13[0]['duration']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we know that our audio's sampling rate are 48000, so we can create a condition to remove audio instances directly using audio arrays' length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize hyperparameters variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters Variables\n",
    "\"\"\"According to the DeepSpeech documentation, \n",
    "a larger beam width value generates better results \n",
    "at the cost of decoding time.\"\"\"\n",
    "beam_width = 100\n",
    "lm_alpha = 0.75\n",
    "lm_beta = 1.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize lm_alpha and lm_beta\n",
    "# https://deepspeech.readthedocs.io/en/v0.9.3/Scorer.html\n",
    "\n",
    "# Code:\n",
    "# Load model into memory\n",
    "model = Model(model_file_path)\n",
    "model.enableExternalScorer(scorer_file_path)\n",
    "\n",
    "# Set hyperparameters\n",
    "model.setScorerAlphaBeta(lm_alpha, lm_beta)\n",
    "model.setBeamWidth(beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio_file):\n",
    "    # Read audio file\n",
    "    with wave.open(audio_file, 'rb') as wav:\n",
    "        rate = wav.getframerate()\n",
    "        frames = wav.getnframes()\n",
    "        buffer = wav.readframes(frames)\n",
    "\n",
    "    # Process audio file\n",
    "    data16 = np.frombuffer(buffer, dtype=np.int16)\n",
    "    return data16, rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(audio_file):\n",
    "    # Process audio file\n",
    "    data16, rate = process_audio(audio_file)\n",
    "\n",
    "    # Transcribe audio file\n",
    "    return model.stt(data16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe(\"data/cv-valid-train/sample-000000.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
